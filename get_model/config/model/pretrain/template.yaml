# config/model/pretrain.yaml
model:
  num_regions: 200
  motif_dim: 639
  embed_dim: 768
  num_layers: 12
  d_model: 768
  nhead: 12
  dropout: 0.1
  output_dim: 1
  pos_emb_components: []
  flash_attn: false

  motif_scanner:
    _target_: get_model.model.modules.MotifScanner
    num_motif: 637
    include_reverse_complement: true
    bidirectional_except_ctcf: true
    motif_prior: true
    learnable: false

  atac_attention:
    _target_: get_model.model.pooling.ATACSplitPool
    pool_method: 'mean'
    atac_kernel_num: 161
    atac_kernel_size: 3
    joint_kernel_num: 161
    joint_kernel_size: 3
    final_bn: false
    atac_input_norm: true

  region_embed:
    _target_: get_model.model.modules.RegionEmbed
    num_regions: ${model.num_regions}
    in_dim: ${model.motif_dim + model.atac_attention.joint_kernel_num}
    out_dim: ${model.embed_dim}

  pos_embed:
    _target_: get_model.model.position_encoding.PositionalEncoding
    d_model: ${model.embed_dim}
    max_len: ${model.num_regions}
    dropout: ${model.dropout}

  encoder:
    _target_: get_model.model.transformer.GETTransformer
    d_model: ${model.embed_dim}
    nhead: ${model.nhead}
    num_layers: ${model.num_layers}
    drop_path_rate: ${model.dropout}
    drop_rate: ${model.dropout}
    attn_drop_rate: ${model.dropout}
    use_mean_pooling: false
    flash_attn: ${model.flash_attn}